{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7baGALVoi2gx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import json\n",
    "import dill\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get static df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lY6PL2Wu3ffD"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_record_df(set_name):\n",
    "  \"\"\"this function loops through the records of the chosen set\"\"\"\n",
    "  full_df_list = []\n",
    "  set_ids = []\n",
    "  for record_file in tqdm(os.listdir('../'+set_name)):\n",
    "        if(record_file=='.DS_Store'):\n",
    "            continue\n",
    "        id = int(record_file.split('.txt')[0])\n",
    "        with open(os.path.join('../'+set_name, record_file), 'r') as file:\n",
    "            df_single_record = pd.read_csv(file)\n",
    "        final_single_record=get_static_variables(df_single_record,id)\n",
    "        full_df_list.append(final_single_record)\n",
    "        set_ids.append(id)\n",
    "  concatendated_fill = (pd.concat(full_df_list, sort=True)).set_index(\"RecordID\")\n",
    "  return(concatendated_fill, set_ids)\n",
    "\n",
    "def get_static_variables( df, id ):\n",
    "        \"\"\"this function extracts the static variables, and stores them in a dataframe\"\"\"\n",
    "        df['Time'] = df['Time'].apply(lambda j: int(j.split(':')[0]))\n",
    "        df = df.pivot_table('Value', 'Time', 'Parameter').reset_index()\n",
    "        df_final = df.iloc[:1]\n",
    "        df_final['RecordID'] = id\n",
    "        df_final[\"ICU_Type\"] = df_final.loc[0, 'ICUType']\n",
    "        df_final= df_final[[\"RecordID\",'Age', 'Gender', 'Height', 'Weight', 'ICU_Type']]\n",
    "        return(df_final)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUytn1OpIlDa"
   },
   "source": [
    "Extract static variables dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TpL-mppi8lUT"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3753fbf1a544ec98a14b88bde078b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunchenxi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/Users/sunchenxi/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00da96cd58b444581ed0126c587ab7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00e882932624c689d3da58f1c13f876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4001), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_set_a,set_a_ids = get_record_df(\"set-a\")\n",
    "df_set_b,set_b_ids = get_record_df(\"set-b\")\n",
    "df_set_c,set_c_ids = get_record_df(\"set-c\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b6MxKvyh-MLq"
   },
   "source": [
    "extract outcomes dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cv3C3mzM-Jnm"
   },
   "outputs": [],
   "source": [
    "outcomes_set_a = (pd.read_csv('../Outcomes-a.txt')).set_index('RecordID')\n",
    "outcomes_set_b =(pd.read_csv('../Outcomes-b.txt')).set_index('RecordID')\n",
    "outcomes_set_c = (pd.read_csv('../Outcomes-c.txt')).set_index('RecordID')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdgIEnJfJ_vE"
   },
   "source": [
    "Export pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zOMMCozz8tfF"
   },
   "outputs": [],
   "source": [
    "with open('extracts/static_set_a.pkl', 'wb') as outfile:\n",
    "    dill.dump(df_set_a, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "with open('extracts/static_set_b.pkl', 'wb') as outfile:\n",
    "    dill.dump(df_set_b, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "with open('extracts/static_set_c.pkl', 'wb') as outfile:\n",
    "    dill.dump(df_set_c, outfile, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "H8hO-sTT_jxc"
   },
   "outputs": [],
   "source": [
    "with open('extracts/ids_set_a.pkl', 'wb') as outfile:\n",
    "    dill.dump(set_a_ids, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "with open('extracts/ids_set_b.pkl', 'wb') as outfile:\n",
    "    dill.dump(set_b_ids, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "with open('extracts/ids_set_c.pkl', 'wb') as outfile:\n",
    "    dill.dump(set_c_ids, outfile, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "K2oHHScGiv0Q"
   },
   "outputs": [],
   "source": [
    "with open('extracts/outcomes_set_a.pkl', 'wb') as outfile:\n",
    "    dill.dump(outcomes_set_a, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "with open('extracts/outcomes_set_b.pkl', 'wb') as outfile:\n",
    "    dill.dump(outcomes_set_b, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "with open('extracts/outcomes_set_c.pkl', 'wb') as outfile:\n",
    "    dill.dump(outcomes_set_c, outfile, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr-VtjiwLi7o"
   },
   "source": [
    "# Get time series df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the physiological time-series variables to be extracted (35 variables)\n",
    "variables = ['ALP','HR', 'DiasABP','Na', 'Lactate', 'NIDiasABP', 'PaO2', 'WBC', 'pH', 'Albumin', 'ALT', 'Glucose', 'SaO2',\n",
    "              'Temp', 'AST', 'Bilirubin', 'BUN', 'RespRate', 'Mg', 'HCT', 'SysABP', 'FiO2', 'K', 'GCS',\n",
    "              'Cholesterol', 'NISysABP', 'TroponinT', 'MAP', 'TroponinI', 'PaCO2', 'Platelets', 'Urine', 'NIMAP',\n",
    "              'Creatinine','HCO3' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interventions(patient_record, intervention_str):\n",
    "    \"\"\" this function gets available intervention adminstration data for the chosen patient record \"\"\"\n",
    "    patient_record = patient_record.set_index('Parameter').to_dict()['Value']\n",
    "    intervention_values = []\n",
    "    for recording in [intervention_str]:\n",
    "        if (recording in patient_record):\n",
    "            intervention_values.append(patient_record[recording])\n",
    "        else:\n",
    "            intervention_values.append(np.nan)\n",
    "    return intervention_values\n",
    "\n",
    "\n",
    "def extract_observations(patient_record, variables):\n",
    "    \"\"\" this function gets available observations for each of the variables /per chosen patient record \"\"\"\n",
    "    data = []\n",
    "    patient_record = patient_record.set_index('Parameter').to_dict()['Value']\n",
    "    for recording in variables:\n",
    "        if (recording in patient_record):\n",
    "            data.append(patient_record[recording])\n",
    "        else:\n",
    "            data.append(np.nan)\n",
    "    return data\n",
    "\n",
    "def group_time_hr(value):\n",
    "    \"\"\" this function groups the observations per hour \"\"\"\n",
    "    hours, _ = map(int, value.split(':'))\n",
    "    return hours\n",
    "\n",
    "def get_dictionary(values, intervention):\n",
    "    \"\"\" this function creates a data dictionary for each of the patient's data \"\"\"\n",
    "    m = pd.DataFrame(values)\n",
    "    dictionary = {}\n",
    "    dictionary[\"intervention\"] = intervention\n",
    "    dictionary['raw'] = values\n",
    "    return dictionary\n",
    "\n",
    "def myconverter(obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, datetime.datetime):\n",
    "            return obj.__str__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_patient_record_ts(id_,set_name,time_length,variables):\n",
    "    \"\"\"this function creates the patient dictionary from the txt files \"\"\"\n",
    "    f_name = '../'+set_name+\"/\"+ str(id_)+\".txt\"\n",
    "    data = pd.read_csv(f_name)\n",
    "    data['Time'] = data['Time'].apply(lambda x: group_time_hr(x))\n",
    "\n",
    "    raw = []\n",
    "    intervention= []\n",
    "    for h in range(time_length):\n",
    "        raw.append(extract_observations(data[data['Time'] == h],variables))\n",
    "        intervention.append(extract_interventions(data[data['Time'] == h],\"MechVent\"))\n",
    "    \n",
    "    raw = np.array(raw)\n",
    "    patient_dictionary = {'id': id_}\n",
    "    patient_dictionary[\"data\"] = get_dictionary(raw,intervention)\n",
    "    patient_dictionary = json.dumps(patient_dictionary,default=myconverter)\n",
    "    return(patient_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_tensors(json_file_name, set_name, time_length, record_ids,variables):\n",
    "    \"\"\" extract the data for the records in the chosen set folder and convert them to tensors\"\"\"\n",
    "    json_file = open(json_file_name, 'w')\n",
    "    for id_ in tqdm(record_ids):\n",
    "            json_file.write(extract_patient_record_ts(id_,set_name,time_length,variables) + '\\n')\n",
    "    json_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tensors(filename):\n",
    "    \"\"\" this function  loads the json files with extracted/formatted data into two components, intervention and physiological data\"\"\"\n",
    "    Data_raw = []\n",
    "    Interventions =[]\n",
    "\n",
    "    for i in open(filename):\n",
    "        data_raw = json.loads(i)[\"data\"][\"raw\"]\n",
    "        interv = json.loads(i)[\"data\"][\"intervention\"]\n",
    "\n",
    "        Interventions.append(interv)\n",
    "        Data_raw.append(data_raw)\n",
    "    \n",
    "    Interventions = np.array(Interventions)\n",
    "    Data_raw =np.array(Data_raw)\n",
    "\n",
    "    return(Data_raw,Interventions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load extracted ids and outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_a = pd.read_pickle(\"extracts/ids_set_a.pkl\")\n",
    "ids_b = pd.read_pickle(\"extracts/ids_set_b.pkl\")\n",
    "ids_c = pd.read_pickle(\"extracts/ids_set_c.pkl\")\n",
    "outcomes_a = pd.read_pickle(\"extracts/outcomes_set_a.pkl\")\n",
    "outcomes_b = pd.read_pickle(\"extracts/outcomes_set_b.pkl\")\n",
    "outcomes_c = pd.read_pickle(\"extracts/outcomes_set_c.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_a.sort()\n",
    "ids_b.sort()\n",
    "ids_c.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps_to_extract = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3677a9bf882f42c28eb7538e3f249356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_to_tensors(\"set_a\",\"set-a\",time_steps_to_extract, ids_a,variables)\n",
    "raw_data_a,interventions_a =load_tensors(\"set_a\")\n",
    "\n",
    "with open('extracts/3d_tensor_set_a.pkl', 'wb') as outfile:\n",
    "    dill.dump(raw_data_a, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "    \n",
    "with open('extracts/interventions_a.pkl', 'wb') as outfile:\n",
    "    dill.dump(interventions_a, outfile, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_tensors(\"set_b\",\"set-b\",time_steps_to_extract, ids_b, variables)\n",
    "\n",
    "raw_data_b,interventions_b =load_tensors(\"set_b\")\n",
    "\n",
    "with open('extracts/3d_tensor_set_b.pkl', 'wb') as outfile:\n",
    "    dill.dump(raw_data_b, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "    \n",
    "with open('extracts/interventions_b.pkl', 'wb') as outfile:\n",
    "    dill.dump(interventions_b, outfile, pickle.HIGHEST_PROTOCOL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_tensors(\"set_c\",\"set-c\",time_steps_to_extract, ids_c,variables)\n",
    "\n",
    "raw_data_c,interventions_c =load_tensors(\"set_c\")\n",
    "\n",
    "with open('extracts/3d_tensor_set_c.pkl', 'wb') as outfile:\n",
    "    dill.dump(raw_data_c, outfile, pickle.HIGHEST_PROTOCOL) \n",
    "    \n",
    "with open('extracts/interventions_c.pkl', 'wb') as outfile:\n",
    "    dill.dump(interventions_c, outfile, pickle.HIGHEST_PROTOCOL) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
